---
title: "Efficient l1-Norm-Based Low-Rank Matrix Approximations for Large-Scale Problems Using Alternating Rectified Gradient Method"
collection: publications
permalink: /publication/TNNLS_2015
excerpt: 'Authors: **Eunwoo Kim**, Minsik Lee, Chong-Ho Choi, Nojun Kwak, and Songhwai Oh'
date: 2015-02-01
venue: 'IEEE Transactions on Neural Networksand Learning Systems (TNNLS)'
---
**Eunwoo Kim**, Minsik Lee, Chong-Ho Choi, Nojun Kwak, and Songhwai Oh, “Efficient l1-Norm-Based Low-Rank Matrix Approximations for  Large-Scale Problems Using Alternating Rectified Gradient Method”, *IEEE Transactions on Neural Networksand Learning Systems (TNNLS)*, vol. 26, no. 2, pp. 237-251, Feb. 2015.

Abstract: Low-rank matrix approximation plays an important role in the area of computer vision and image processing. Most of the conventional low-rank matrix approximation methods are based on the l2-norm (Frobenius norm) with principal component analysis (PCA) being the most popular among them. However, this can give a poor approximation for data contaminated by outliers (including missing data), because the l2-norm exaggerates the negative effect of outliers. Recently, to overcome this problem, various methods based on the l1-norm, such as robust PCA methods, have been proposed for low-rank matrix approximation. Despite the robustness of the methods, they require heavy computational effort and substantial memory for high-dimensional data, which is impractical for realworld problems. In this paper, we propose two efficient low-rank factorization methods based on the l1-norm that find proper projection and coefficient matrices using the alternating rectified gradient method. The proposed methods are applied to a number of low-rank matrix approximation problems to demonstrate their efficiency and robustness. The experimental results show that our proposals are efficient in both execution time and reconstruction performance unlike other state-of-the-art methods.

[[Paper](https://ieeexplore.ieee.org/abstract/document/6784021)] [Code]
